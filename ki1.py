# -*- coding: utf-8 -*-
"""Flexible UDE Degradation Modeler

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U8TLBBrYwz88IQLJqiULX7Q7JwkDF8BT
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import solve_ivp
# Note: curve_fit/linregress are not used for fitting here, only UDE
# from scipy.optimize import curve_fit
# from scipy.stats import linregress
import torch
import torch.nn as nn
import torch.optim as optim
import time
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import sys
import math # For log in AIC/BIC

# --- Constants ---
R = 8.314  # J/(mol*K)

# --- !!! USER INPUTS !!! ---
excel_file_path = "/content/hplc_data_table.xlsx" # <<< CHANGE THIS to your data file
# Choose the degradation model to use:
# Options: 'zero_order', 'first_order', 'second_order', 'nth_order', 'prout_tompkins'
MODEL_CHOICE = 'first_order'
TRAIN_UP_TO_DAY = 90 # Use data up to this day for training
EXTRAPOLATE_TO_DAY = 1095 # Extrapolate predictions up to 3 years
# ---

# --- Read Data From Excel ---
try:
    print(f"--- Reading Data From Excel: {excel_file_path} ---")
    data_df = pd.read_excel(excel_file_path)
    print(f"Successfully read {len(data_df)} data points.")
except FileNotFoundError:
    print(f"ERROR: Excel file not found at '{excel_file_path}'.")
    sys.exit(f"Exiting: File '{excel_file_path}' not found.")
except Exception as e:
    print(f"ERROR: Could not read Excel file. Error: {e}")
    sys.exit(f"Exiting: Error reading file.")

# --- Data Preparation ---
# <<< ADJUST column mapping if your headers are different >>>
column_mapping = {
    'Experiment': 'batch',
    'Temperature_C': 'T_c',
    'Time_days': 'time',
    'Purity': 'concentration' # Assuming Purity/Concentration is in 'Purity' column
}
missing_cols = [col for col in column_mapping.keys() if col not in data_df.columns]
if missing_cols:
    print(f"ERROR: Missing required columns: {missing_cols}. Expected: {list(column_mapping.keys())}")
    sys.exit("Exiting: Missing columns.")

data_df = data_df.rename(columns=column_mapping)
data_df['T_k'] = data_df['T_c'] + 273.15

# Determine initial concentration (y_initial) from data at time = 0
time_zero_data = data_df[data_df['time'] == 0]['concentration']
if time_zero_data.empty:
    print("WARNING: No data found at Time_days = 0. Assuming default y_initial = 100.0")
    y_initial = 100.0
else:
    y_initial = time_zero_data.mean()
    print(f"Determined y_initial (mean at time=0): {y_initial:.3f}")
    if time_zero_data.std() > 1: # Arbitrary threshold for warning
        print(f"WARNING: High std dev ({time_zero_data.std():.2f}) in initial values.")

numeric_cols = ['T_c', 'time', 'concentration', 'T_k']
for col in numeric_cols: data_df[col] = pd.to_numeric(data_df[col], errors='coerce')
initial_rows = len(data_df)
data_df.dropna(subset=numeric_cols, inplace=True)
if len(data_df) < initial_rows: print(f"WARNING: Dropped {initial_rows - len(data_df)} rows due to non-numeric data.")
if data_df.empty: sys.exit("Exiting: No valid numeric data.")

# --- Split Data for Training ---
train_df = data_df[data_df['time'] <= TRAIN_UP_TO_DAY].copy()
if train_df.empty: sys.exit(f"Exiting: No data available for training (time <= {TRAIN_UP_TO_DAY} days).")
print(f"Using {len(train_df)} data points for training (up to day {TRAIN_UP_TO_DAY}).")

# Prepare training data grouped by temperature
train_data_grouped_by_temp = {}
T_kelvin_train = np.sort(train_df['T_k'].unique())
for T_k in T_kelvin_train:
    temp_df = train_df[train_df['T_k'] == T_k]
    if not temp_df.empty:
        train_data_grouped_by_temp[T_k] = {
            'time_days': torch.tensor(temp_df['time'].values, dtype=torch.float32),
            'concentration': torch.tensor(temp_df['concentration'].values, dtype=torch.float32)
        }
print(f"Training data prepared for {len(train_data_grouped_by_temp)} unique temperatures.")
print("-" * 30)
y0_tensor = torch.tensor([y_initial], dtype=torch.float32)

# --- Define ODE Functions for Different Models ---

def zero_order_ode_torch(y, t, logA, logEa, T_k):
    """ dy/dt = -k """
    A = torch.exp(logA)
    Ea = torch.exp(logEa)
    T_k_val = T_k.item() if isinstance(T_k, torch.Tensor) else T_k
    k = A * torch.exp(-Ea / (R * T_k_val))
    return -k * torch.ones_like(y) # Rate is constant

def first_order_ode_torch(y, t, logA, logEa, T_k):
    """ dy/dt = -k * y """
    A = torch.exp(logA)
    Ea = torch.exp(logEa)
    T_k_val = T_k.item() if isinstance(T_k, torch.Tensor) else T_k
    k = A * torch.exp(-Ea / (R * T_k_val))
    y_clamped = torch.clamp(y, min=0.0) # Ensure y >= 0
    return -k * y_clamped

def second_order_ode_torch(y, t, logA, logEa, T_k):
    """ dy/dt = -k * y^2 """
    A = torch.exp(logA)
    Ea = torch.exp(logEa)
    T_k_val = T_k.item() if isinstance(T_k, torch.Tensor) else T_k
    k = A * torch.exp(-Ea / (R * T_k_val))
    y_clamped = torch.clamp(y, min=0.0)
    return -k * torch.pow(y_clamped, 2)

def nth_order_ode_torch(y, t, logA, logEa, n_param, T_k):
    """ dy/dt = -k * y^n """
    A = torch.exp(logA)
    Ea = torch.exp(logEa)
    T_k_val = T_k.item() if isinstance(T_k, torch.Tensor) else T_k
    k = A * torch.exp(-Ea / (R * T_k_val))
    y_clamped = torch.clamp(y, min=1e-9) # Clamp slightly above zero for pow
    return -k * torch.pow(y_clamped, n_param)

def prout_tompkins_ode_torch(y, t, logA, logEa, T_k, y0):
    """ dy/dt = -(k/y0) * y * (y0 - y) """
    A = torch.exp(logA)
    Ea = torch.exp(logEa)
    T_k_val = T_k.item() if isinstance(T_k, torch.Tensor) else T_k
    k = A * torch.exp(-Ea / (R * T_k_val))
    y_clamped = torch.clamp(y, min=0.0)
    y0_val = y0.item() # Get scalar value of initial concentration
    # Clamp (y0 - y) to avoid negative result if y exceeds y0 numerically
    term2 = torch.clamp(y0_val - y_clamped, min=0.0)
    return -(k / y0_val) * y_clamped * term2

# --- Model Configuration ---
MODEL_CONFIG = {
    'zero_order': {'func': zero_order_ode_torch, 'params': ['logA', 'logEa']},
    'first_order': {'func': first_order_ode_torch, 'params': ['logA', 'logEa']},
    'second_order': {'func': second_order_ode_torch, 'params': ['logA', 'logEa']},
    'nth_order': {'func': nth_order_ode_torch, 'params': ['logA', 'logEa', 'n']},
    'prout_tompkins': {'func': prout_tompkins_ode_torch, 'params': ['logA', 'logEa']},
}

if MODEL_CHOICE not in MODEL_CONFIG:
    raise ValueError(f"Invalid MODEL_CHOICE: '{MODEL_CHOICE}'. Options are: {list(MODEL_CONFIG.keys())}")

selected_ode_func = MODEL_CONFIG[MODEL_CHOICE]['func']
selected_params_names = MODEL_CONFIG[MODEL_CHOICE]['params']
num_estimated_params = len(selected_params_names) # For AIC/BIC

print(f"Selected Model: {MODEL_CHOICE}")
print(f"Parameters to estimate: {selected_params_names}")

# --- Differentiable ODE Solver (RK4) ---
# solve_ode_rk4_torch function remains the same as in previous versions
def solve_ode_rk4_torch(ode_func, y0, t_eval, ode_params):
    """ Solves ODE using RK4, maintaining differentiability wrt ode_params. """
    y_pred_list = []
    y_i = y0.clone()
    # Ensure t_eval is sorted for correct dt calculation and handling
    t_eval_sorted, sort_indices = torch.sort(t_eval)
    dts = t_eval_sorted[1:] - t_eval_sorted[:-1]
    y_pred_dict = {t_eval_sorted[0].item(): y_i}

    for i in range(len(t_eval_sorted) - 1):
        t_i = t_eval_sorted[i]
        dt = dts[i]
        if dt <= 1e-9: # Use a small threshold for duplicate times
             y_pred_dict[t_eval_sorted[i+1].item()] = y_i
             continue

        # Pass the required parameters (*ode_params expands the tuple)
        k1 = ode_func(y_i, t_i, *ode_params)
        k2 = ode_func(y_i + 0.5 * dt * k1, t_i + 0.5 * dt, *ode_params)
        k3 = ode_func(y_i + 0.5 * dt * k2, t_i + 0.5 * dt, *ode_params)
        k4 = ode_func(y_i + dt * k3, t_i + dt, *ode_params)

        y_i = y_i + (dt / 6.0) * (k1 + 2.0*k2 + 2.0*k3 + k4)
        y_i = torch.clamp(y_i, min=0.0) # Ensure non-negative
        y_pred_dict[t_eval_sorted[i+1].item()] = y_i
    try:
        # Reorder predictions based on the original t_eval order
        y_pred_final = [y_pred_dict[t.item()] for t in t_eval]
    except KeyError as e:
         print(f"KeyError during prediction reordering: {e}.")
         y_pred_final = list(y_pred_dict.values())
         if len(y_pred_final) != len(t_eval):
              print("ERROR: Prediction length mismatch. Returning empty tensor.")
              return torch.tensor([]) # Return empty tensor on error
    return torch.stack(y_pred_final)


# --- Training Function (Handles Different Models) ---
def train_ude_parameters(model_config, initial_guesses, data_grouped_by_temp, y0_tensor, epochs=3000, lr=0.01):
    """ Trains parameters for the selected ODE model. """
    params_to_learn = []
    param_dict = {}

    # Initialize parameters based on model config
    if 'logA' in model_config['params']:
        logA = nn.Parameter(torch.tensor(initial_guesses['logA'], dtype=torch.float32))
        params_to_learn.append(logA)
        param_dict['logA'] = logA
    if 'logEa' in model_config['params']:
        logEa = nn.Parameter(torch.tensor(initial_guesses['logEa'], dtype=torch.float32))
        params_to_learn.append(logEa)
        param_dict['logEa'] = logEa
    if 'n' in model_config['params']:
        n_param = nn.Parameter(torch.tensor(initial_guesses['n'], dtype=torch.float32))
        params_to_learn.append(n_param)
        param_dict['n'] = n_param
    # Add other parameters like y_max here if needed for future models

    if not params_to_learn:
        raise ValueError("No parameters specified for learning in model_config.")

    optimizer = optim.Adam(params_to_learn, lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=750, gamma=0.5) # Adjusted step_size
    losses = []
    ode_func = model_config['func']

    print(f"--- Starting UDE Training ({MODEL_CHOICE} model) ---")
    start_time = time.time()

    for epoch in range(epochs):
        optimizer.zero_grad()
        total_loss = 0.0
        total_points = 0

        for T_k, temp_data in data_grouped_by_temp.items():
            t_all = temp_data['time_days']
            y_target = temp_data['concentration']
            T_k_tensor = torch.tensor(T_k, dtype=torch.float32)

            # Construct ode_params tuple dynamically based on the model's requirements
            current_ode_params_list = []
            if 'logA' in param_dict: current_ode_params_list.append(param_dict['logA'])
            if 'logEa' in param_dict: current_ode_params_list.append(param_dict['logEa'])
            if 'n' in param_dict: current_ode_params_list.append(param_dict['n'])
            # Always add T_k
            current_ode_params_list.append(T_k_tensor)
            # Add y0 only if required by the specific ODE function (e.g., Prout-Tompkins)
            if ode_func == prout_tompkins_ode_torch:
                 current_ode_params_list.append(y0_tensor)

            current_ode_params = tuple(current_ode_params_list)

            # Solve ODE
            y_pred = solve_ode_rk4_torch(ode_func, y0_tensor.clone(), t_all, ode_params=current_ode_params)

            if y_pred.numel() == 0: continue
            if y_pred.squeeze().shape != y_target.shape: continue

            loss = torch.sum((y_pred.squeeze() - y_target)**2)
            total_loss = total_loss + loss
            total_points += len(y_target)

        if total_points == 0: break
        avg_loss = total_loss / total_points
        losses.append(avg_loss.item())

        if torch.isnan(avg_loss) or torch.isinf(avg_loss):
            print(f"WARNING: Loss is {avg_loss.item()} at epoch {epoch+1}. Stopping training.")
            break

        avg_loss.backward()
        torch.nn.utils.clip_grad_norm_(params_to_learn, max_norm=1.0)
        optimizer.step()
        scheduler.step()

        if (epoch + 1) % 200 == 0:
             print_str = f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss.item():.6f}, LR: {scheduler.get_last_lr()[0]:.6f}'
             if 'logA' in param_dict: print_str += f', A: {torch.exp(param_dict["logA"]).item():.2e}'
             if 'logEa' in param_dict: print_str += f', Ea: {torch.exp(param_dict["logEa"]).item():.2f}'
             if 'n' in param_dict: print_str += f', n: {param_dict["n"].item():.3f}'
             print(print_str)

    end_time = time.time()
    print(f"Training finished in {end_time - start_time:.2f} seconds.")

    # Prepare final parameters dictionary
    final_params = {}
    fit_successful = True
    for name, param_tensor in param_dict.items():
         final_params[name] = param_tensor.detach().clone()
         if torch.isnan(final_params[name]): fit_successful = False

    if not fit_successful: print("WARNING: Final parameters contain NaN values. Fit may have failed.")
    else: print(f"Final UDE Params -> { {k: (torch.exp(v) if 'log' in k else v).item() for k, v in final_params.items()} }")

    print("-" * 30)
    return final_params, losses, fit_successful


# --- Evaluation Metrics Calculation ---
def get_predictions_and_actuals(final_params, model_config, data_df, y0_tensor):
    """Generates predictions using RK4 for all data points."""
    all_preds = []
    all_actuals = []
    ode_func = model_config['func']

    grouped = data_df.groupby('T_k')
    with torch.no_grad():
        for T_k, group in grouped:
            t_eval = torch.tensor(group['time'].values, dtype=torch.float32)
            y_actual_np = group['concentration'].values
            T_k_tensor = torch.tensor(T_k, dtype=torch.float32)

            # Construct ode_params tuple dynamically
            current_ode_params_list = []
            # Ensure order matches the specific ode_func signature
            if 'logA' in final_params: current_ode_params_list.append(final_params['logA'])
            if 'logEa' in final_params: current_ode_params_list.append(final_params['logEa'])
            if 'n' in final_params: current_ode_params_list.append(final_params['n'])
            current_ode_params_list.append(T_k_tensor)
            if ode_func == prout_tompkins_ode_torch:
                 current_ode_params_list.append(y0_tensor)
            ode_params = tuple(current_ode_params_list)

            y_pred_torch = solve_ode_rk4_torch(ode_func, y0_tensor.clone(), t_eval, ode_params=ode_params)

            if y_pred_torch.numel() == 0 or y_pred_torch.squeeze().shape != torch.tensor(y_actual_np).shape:
                 print(f"WARNING: Solver failed or shape mismatch for T={T_k} during prediction generation. Skipping.")
                 continue

            y_pred_np = y_pred_torch.squeeze().cpu().numpy()
            valid_idx = ~np.isnan(y_pred_np) # Filter out NaNs if any

            all_preds.extend(y_pred_np[valid_idx])
            all_actuals.extend(y_actual_np[valid_idx])

    return np.array(all_actuals), np.array(all_preds)


def calculate_fit_metrics(y_actual, y_predicted, num_observations, num_params):
    """ Calculates R2, RMSE, AIC, BIC """
    if len(y_actual) < 2 or len(y_predicted) < 2 or len(y_actual) != len(y_predicted):
        print("Insufficient data points for metric calculation.")
        return {'R2': np.nan, 'RMSE': np.nan, 'AIC': np.nan, 'BIC': np.nan}

    try:
        r2 = r2_score(y_actual, y_predicted)
        mse = mean_squared_error(y_actual, y_predicted)
        rmse = np.sqrt(mse)

        # Calculate AIC and BIC
        # Using formula for least squares assuming normal errors
        # AIC = n * ln(RSS/n) + 2*p
        # BIC = n * ln(RSS/n) + p*ln(n)
        rss = mse * num_observations
        if rss <= 0: # Avoid log(0) or log(negative)
             aic = np.inf
             bic = np.inf
        else:
             log_likelihood_term = num_observations * math.log(rss / num_observations)
             aic = log_likelihood_term + 2 * num_params
             bic = log_likelihood_term + num_params * math.log(num_observations)

        metrics = {'R2': r2, 'RMSE': rmse, 'AIC': aic, 'BIC': bic}
        print("Calculated Metrics:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.4f}")
        return metrics

    except Exception as e:
        print(f"Error calculating metrics: {e}")
        return {'R2': np.nan, 'RMSE': np.nan, 'AIC': np.nan, 'BIC': np.nan}


# --- Main Execution ---
if __name__ == '__main__':
    if data_df.empty:
        print("Exiting due to data loading/preparation errors.")
    else:
        # --- UDE Parameter Fitting ---
        # Set initial guesses (adjust based on expected model/data)
        initial_guesses = {
            'logA': np.log(1e11),
            'logEa': np.log(75000),
            'n': 1.0 # Default guess for n if needed
            # Add other params like 'y_max' here if needed
        }
        print(f"Using Initial Guesses: { {k: f'{v:.2f}' if k != 'logA' else f'{np.exp(v):.1e}' for k,v in initial_guesses.items()} }")

        # Select only the needed guesses for the chosen model
        current_initial_guesses = {k: initial_guesses[k] for k in MODEL_CONFIG[MODEL_CHOICE]['params']}

        final_params, ude_losses, fit_successful = train_ude_parameters(
            MODEL_CONFIG[MODEL_CHOICE],
            current_initial_guesses,
            train_data_grouped_by_temp, # Train only on data <= 90 days
            y0_tensor,
            epochs=5000, lr=0.002 # Adjust epochs/lr
        )

        # --- Evaluation Metrics Calculation (on Training Data) ---
        print("\n--- Goodness of Fit (on Training Data) ---")
        metrics = {'R2': np.nan, 'RMSE': np.nan, 'AIC': np.nan, 'BIC': np.nan} # Default
        if fit_successful:
            # Get predictions specifically for the training data points
            y_actual_train, y_pred_train = get_predictions_and_actuals(
                final_params, MODEL_CONFIG[MODEL_CHOICE], train_df, y0_tensor
            )
            if len(y_actual_train) > num_estimated_params: # Need more points than params
                 metrics = calculate_fit_metrics(
                     y_actual_train, y_pred_train,
                     num_observations=len(y_actual_train),
                     num_params=num_estimated_params
                 )
            else:
                 print("Not enough training data points relative to model parameters to calculate AIC/BIC reliably.")
        else:
             print("Fit failed, skipping metric calculation.")
        print("-" * 30)

        # --- Parameter Report ---
        print("\n--- Final Parameter Estimates ---")
        print(f"Model: {MODEL_CHOICE}")
        if fit_successful:
             for name, tensor_val in final_params.items():
                  value = torch.exp(tensor_val).item() if 'log' in name else tensor_val.item()
                  unit = "(J/mol)" if name == 'logEa' else "(rate units)" if name == 'logA' else ""
                  print(f"{name.replace('log','').replace('_param',''):<10} | {value:.4e} {unit}")
        else:
             print("Fit failed to produce valid parameters.")
        print("-" * 30)


        # --- Plotting (Extrapolated) ---
        print("--- Generating Plot ---")
        plt.figure(figsize=(14, 9))
        unique_temps_k_all = np.sort(data_df['T_k'].unique()) # Use all temps for plotting
        colors = plt.cm.viridis(np.linspace(0, 1, len(unique_temps_k_all)))
        temp_color_map = {T_k: colors[i] for i, T_k in enumerate(unique_temps_k_all)}
        batch_markers = ['o', 's', '^', 'D', 'v', '<', '>']

        # Time range for extrapolation plot
        t_extrapolate_dense = np.linspace(0, EXTRAPOLATE_TO_DAY, 200)

        # Plot ALL data points (including those > TRAIN_UP_TO_DAY)
        batches = data_df['batch'].unique()
        for i, batch_id in enumerate(batches):
            batch_df = data_df[data_df['batch'] == batch_id]
            marker = batch_markers[i % len(batch_markers)]
            for T_k, color in temp_color_map.items():
                temp_batc